{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7VGBuoyEdNP6"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "import shutil, os\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "DATA_DIR = \"/content/drive/MyDrive\"\n",
        "os.chdir(\"/content\")\n",
        "\n",
        "for fname in [\"train_guide.csv\", \"test_guide.csv\", \"building_info_guide.csv\", \"sample_submission guide.csv\"]:\n",
        "    src = os.path.join(DATA_DIR, fname)\n",
        "    dst = os.path.join(\"/content\", fname)\n",
        "    shutil.copy2(src, dst)\n",
        "    print(f\"[COPIED] {src} -> {dst}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install xgboost optuna\n",
        "\n",
        "import os, gc, warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import optuna\n",
        "from xgboost import XGBRegressor\n",
        "\n",
        "SEED = 42\n",
        "np.random.seed(SEED)\n",
        "\n",
        "PATH_TRAIN = \"train_guide.csv\"\n",
        "PATH_TEST  = \"test_guide.csv\"\n",
        "PATH_INFO  = \"building_info_guide.csv\"\n",
        "PATH_SUB_CANDIDATES = [\"sample_submission_guide.csv\", \"sample_submission guide.csv\"]\n",
        "\n",
        "\n",
        "# 설정 (가중치/이상치/포리에/CDH)\n",
        "PEAK_HOURS = list(range(9, 19))   # 9~18시 inclusive\n",
        "PEAK_WEIGHT = 2.0\n",
        "\n",
        "ROLL_WIN_H = 24\n",
        "MAD_K = 5.0\n",
        "RUN_INTERP_MAX = 2\n",
        "RUN_DROP_MIN  = 3\n",
        "\n",
        "DAILY_K  = 3\n",
        "WEEKLY_K = 2\n",
        "\n",
        "CDH_BASE_TEMP = 26\n",
        "CDH_WINDOW_H  = 12\n",
        "\n",
        "TARGET = \"전력소비량(kWh)\"\n",
        "\n",
        "\n",
        "# 유틸\n",
        "def to_datetime_yyyymmdd_hh(s):\n",
        "    s = s.astype(str).str.strip()\n",
        "    y = s.str.slice(0,4).astype(int)\n",
        "    m = s.str.slice(4,6).astype(int)\n",
        "    d = s.str.slice(8,10).astype(int) if ':' in s.iloc[0] else s.str.slice(6,8).astype(int)\n",
        "    hh = s.str.slice(9,11).astype(int) if ':' not in s.iloc[0] else s.str.slice(11,13).astype(int)\n",
        "    return pd.to_datetime(dict(year=y, month=m, day=d, hour=hh))\n",
        "\n",
        "def smape(y_true, y_pred):\n",
        "    y_true = np.asarray(y_true, dtype=float)\n",
        "    y_pred = np.asarray(y_pred, dtype=float)\n",
        "    denom = np.abs(y_true) + np.abs(y_pred)\n",
        "    out = 2.0 * np.abs(y_pred - y_true) / np.maximum(denom, 1e-9)\n",
        "    return 100.0 * np.mean(out)\n",
        "\n",
        "def add_time_features(df, ts_col=\"일시_dt\"):\n",
        "    df[\"hour\"] = df[ts_col].dt.hour\n",
        "    df[\"weekday\"] = df[ts_col].dt.weekday\n",
        "    df[\"is_weekend\"] = (df[\"weekday\"] >= 5).astype(int)\n",
        "    df[\"sin_hour\"] = np.sin(2*np.pi*df[\"hour\"]/24.0)\n",
        "    df[\"cos_hour\"] = np.cos(2*np.pi*df[\"hour\"]/24.0)\n",
        "    return df\n",
        "\n",
        "def add_periodic_fourier(df, ts_col=\"일시_dt\",\n",
        "                         daily_period=24, daily_K=3,\n",
        "                         weekly_period=24*7, weekly_K=2):\n",
        "    hour = df[ts_col].dt.hour\n",
        "    dow = df[ts_col].dt.weekday\n",
        "    hour_of_week = (dow * 24 + hour).astype(int)  # 0~167\n",
        "    for k in range(1, daily_K+1):\n",
        "        df[f\"fourier_day_sin_k{k}\"] = np.sin(2*np.pi*k*hour/daily_period)\n",
        "        df[f\"fourier_day_cos_k{k}\"] = np.cos(2*np.pi*k*hour/daily_period)\n",
        "    for k in range(1, weekly_K+1):\n",
        "        df[f\"fourier_week_sin_k{k}\"] = np.sin(2*np.pi*k*hour_of_week/weekly_period)\n",
        "        df[f\"fourier_week_cos_k{k}\"] = np.cos(2*np.pi*k*hour_of_week/weekly_period)\n",
        "    df[\"hour_of_week\"] = hour_of_week\n",
        "    return df\n",
        "\n",
        "def stull_wet_bulb(Ta, RH):\n",
        "    Ta = np.asarray(Ta, dtype=float)\n",
        "    RH = np.asarray(RH, dtype=float)\n",
        "    return (Ta*np.arctan(0.151977*np.sqrt(RH+8.313659))\n",
        "            + np.arctan(Ta+RH)\n",
        "            - np.arctan(RH-1.67633)\n",
        "            + 0.00391838*(RH**1.5)*np.arctan(0.023101*RH)\n",
        "            - 4.686035)\n",
        "\n",
        "def heat_index_from_Ta_RH(Ta, RH):\n",
        "    Tw = stull_wet_bulb(Ta, RH)\n",
        "    return (-0.2442 + 0.55399*Tw + 0.45535*Ta - 0.0022*(Tw**2) + 0.00278*(Tw*Ta) + 3.0)\n",
        "\n",
        "def safe_numeric(df, cols):\n",
        "    for c in cols:\n",
        "        if c in df.columns:\n",
        "            df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
        "    return df\n",
        "\n",
        "# -------- 이상치 처리 --------\n",
        "def _mad(series):\n",
        "    med = series.median()\n",
        "    return np.median(np.abs(series - med))\n",
        "\n",
        "def detect_outlier_mask(y, window=24, k=5.0):\n",
        "    s = pd.Series(y).astype(float)\n",
        "    roll_med = s.rolling(window=window, center=True, min_periods=max(3, window//3)).median()\n",
        "    resid = s - roll_med\n",
        "    roll_mad = resid.rolling(window=window, center=True, min_periods=max(3, window//3)).apply(_mad, raw=False)\n",
        "    global_mad = _mad(resid.dropna()) or 1e-6\n",
        "    roll_mad = roll_mad.fillna(global_mad)\n",
        "    z = np.abs(resid) / (1.4826 * roll_mad + 1e-6)\n",
        "    return (z > k).values\n",
        "\n",
        "def split_runs(bool_mask):\n",
        "    idx = np.where(bool_mask)[0]\n",
        "    if len(idx) == 0:\n",
        "        return []\n",
        "    runs, start, prev = [], idx[0], idx[0]\n",
        "    for i in idx[1:]:\n",
        "        if i == prev + 1:\n",
        "            prev = i\n",
        "        else:\n",
        "            runs.append((start, prev))\n",
        "            start = i\n",
        "            prev = i\n",
        "    runs.append((start, prev))\n",
        "    return runs\n",
        "\n",
        "def repair_or_drop_block(series, start, end, strategy=\"interp\"):\n",
        "    s = series.copy()\n",
        "    if strategy == \"interp\":\n",
        "        s.iloc[start:end+1] = np.nan\n",
        "        s = s.interpolate(method=\"linear\", limit_direction=\"both\")\n",
        "    return s\n",
        "\n",
        "def clean_outliers_per_building(df, bid_col, ycol, ts_col,\n",
        "                               roll_win=24, k=5.0,\n",
        "                               run_interp_max=2, run_drop_min=3):\n",
        "    out_frames, drop_indices = [], []\n",
        "    for bid, g in df.sort_values(ts_col).groupby(bid_col, sort=False):\n",
        "        g = g.copy()\n",
        "        y = g[ycol].astype(float).values\n",
        "        mask = detect_outlier_mask(y, window=roll_win, k=k)\n",
        "        runs = split_runs(mask)\n",
        "        if not runs:\n",
        "            out_frames.append(g); continue\n",
        "        s = g[ycol].astype(float)\n",
        "        for (st, ed) in runs:\n",
        "            length = ed - st + 1\n",
        "            if length <= run_interp_max:\n",
        "                s = repair_or_drop_block(s, st, ed, strategy=\"interp\")\n",
        "            elif length >= run_drop_min:\n",
        "                drop_indices.extend(g.iloc[st:ed+1].index.tolist())\n",
        "            else:\n",
        "                s = repair_or_drop_block(s, st, ed, strategy=\"interp\")\n",
        "        g[ycol] = s.values\n",
        "        out_frames.append(g)\n",
        "    out_df = pd.concat(out_frames, axis=0)\n",
        "    if drop_indices:\n",
        "        out_df = out_df.drop(index=drop_indices)\n",
        "    return out_df.reset_index(drop=True)\n",
        "\n",
        "# -------- CDH(누적 냉방부하)\n",
        "def compute_cdh_array(xs, base_temp=26, window=12):\n",
        "    ys = []\n",
        "    for i in range(len(xs)):\n",
        "        if i < window - 1:\n",
        "            window_vals = xs[:(i+1)]\n",
        "        else:\n",
        "            window_vals = xs[(i - window + 1):(i + 1)]\n",
        "        excess = np.maximum(0, window_vals - base_temp)\n",
        "        ys.append(np.sum(excess))\n",
        "    return np.array(ys)\n",
        "\n",
        "def add_cdh_indicator(df, temp_col='기온(°C)', group_col='건물번호', base_temp=26, window=12):\n",
        "    df = df.copy()\n",
        "    def apply_cdh(group):\n",
        "        temps = group[temp_col].values\n",
        "        cdh_vals = compute_cdh_array(temps, base_temp=base_temp, window=window)\n",
        "        return pd.Series(cdh_vals, index=group.index)\n",
        "    df['CDH'] = df.groupby(group_col).apply(apply_cdh).reset_index(level=0, drop=True)\n",
        "    return df\n",
        "\n",
        "\n",
        "# 1)Load\n",
        "train = pd.read_csv(PATH_TRAIN)\n",
        "test  = pd.read_csv(PATH_TEST)\n",
        "info  = pd.read_csv(PATH_INFO)\n",
        "\n",
        "sub = None\n",
        "for cand in PATH_SUB_CANDIDATES:\n",
        "    if os.path.exists(cand):\n",
        "        sub = pd.read_csv(cand); print(f\"[INFO] Loaded submission template: {cand}\"); break\n",
        "if sub is None:\n",
        "    raise FileNotFoundError(\"sample_submission 파일명을 확인하세요: sample_submission_guide.csv 또는 sample_submission guide.csv\")\n",
        "\n",
        "\n",
        "# 2) Preprocess\n",
        "info = info.replace(\"-\", 0)\n",
        "info = safe_numeric(info, [\"연면적(m2)\", \"냉방면적(m2)\", \"태양광용량(kW)\", \"ESS저장용량(kWh)\", \"PCS용량(kW)\"])\n",
        "\n",
        "train[\"일시_dt\"] = to_datetime_yyyymmdd_hh(train[\"일시\"])\n",
        "test[\"일시_dt\"]  = to_datetime_yyyymmdd_hh(test[\"일시\"])\n",
        "\n",
        "# join by 건물번호\n",
        "train = train.merge(info, on=\"건물번호\", how=\"left\")\n",
        "test  = test.merge(info, on=\"건물번호\", how=\"left\")\n",
        "\n",
        "\n",
        "# 3) 이상치 처리 (train만)\n",
        "train = clean_outliers_per_building(\n",
        "    df=train,\n",
        "    bid_col=\"건물번호\", ycol=TARGET, ts_col=\"일시_dt\",\n",
        "    roll_win=ROLL_WIN_H, k=MAD_K,\n",
        "    run_interp_max=RUN_INTERP_MAX, run_drop_min=RUN_DROP_MIN\n",
        ")\n",
        "\n",
        "\n",
        "# 4) Feature engineering\n",
        "\n",
        "# 시간 파생 + Fourier\n",
        "train = add_time_features(train, \"일시_dt\")\n",
        "test  = add_time_features(test, \"일시_dt\")\n",
        "train = add_periodic_fourier(train, \"일시_dt\", daily_K=DAILY_K, weekly_K=WEEKLY_K)\n",
        "test  = add_periodic_fourier(test,  \"일시_dt\", daily_K=DAILY_K, weekly_K=WEEKLY_K)\n",
        "\n",
        "# 체감온도\n",
        "def heat_index_from_Ta_RH(Ta, RH):\n",
        "    Tw = stull_wet_bulb(Ta, RH)\n",
        "    return (-0.2442 + 0.55399*Tw + 0.45535*Ta - 0.0022*(Tw**2) + 0.00278*(Tw*Ta) + 3.0)\n",
        "\n",
        "def stull_wet_bulb(Ta, RH):\n",
        "    Ta = np.asarray(Ta, dtype=float)\n",
        "    RH = np.asarray(RH, dtype=float)\n",
        "    return (Ta*np.arctan(0.151977*np.sqrt(RH+8.313659))\n",
        "            + np.arctan(Ta+RH)\n",
        "            - np.arctan(RH-1.67633)\n",
        "            + 0.00391838*(RH**1.5)*np.arctan(0.023101*RH)\n",
        "            - 4.686035)\n",
        "\n",
        "train[\"체감온도\"] = heat_index_from_Ta_RH(train[\"기온(°C)\"], train[\"습도(%)\"])\n",
        "test[\"체감온도\"]  = heat_index_from_Ta_RH(test[\"기온(°C)\"],  test[\"습도(%)\"])\n",
        "\n",
        "# --- CDH(누적 냉방부하) ---\n",
        "train = add_cdh_indicator(train, temp_col='기온(°C)', group_col='건물번호',\n",
        "                          base_temp=CDH_BASE_TEMP, window=CDH_WINDOW_H)\n",
        "test  = add_cdh_indicator(test,  temp_col='기온(°C)', group_col='건물번호',\n",
        "                          base_temp=CDH_BASE_TEMP, window=CDH_WINDOW_H)\n",
        "\n",
        "# --- 집단통계 ---\n",
        "grp_type_hour = (train\n",
        "                 .groupby([\"건물유형\",\"hour\"])[TARGET]\n",
        "                 .agg(type_hour_mean=\"mean\", type_hour_std=\"std\")\n",
        "                 .reset_index())\n",
        "train = train.merge(grp_type_hour, on=[\"건물유형\",\"hour\"], how=\"left\")\n",
        "test  = test.merge(grp_type_hour,  on=[\"건물유형\",\"hour\"], how=\"left\")\n",
        "\n",
        "grp_type_wkd_hour = (train\n",
        "                     .groupby([\"건물유형\",\"is_weekend\",\"hour\"])[TARGET]\n",
        "                     .agg(type_wkd_hour_mean=\"mean\", type_wkd_hour_std=\"std\")\n",
        "                     .reset_index())\n",
        "train = train.merge(grp_type_wkd_hour, on=[\"건물유형\",\"is_weekend\",\"hour\"], how=\"left\")\n",
        "test  = test.merge(grp_type_wkd_hour,  on=[\"건물유형\",\"is_weekend\",\"hour\"], how=\"left\")\n",
        "\n",
        "grp_bld = (train.groupby(\"건물번호\")[TARGET]\n",
        "           .agg(bld_mean=\"mean\", bld_std=\"std\")\n",
        "           .reset_index())\n",
        "train = train.merge(grp_bld, on=\"건물번호\", how=\"left\")\n",
        "test  = test.merge(grp_bld,  on=\"건물번호\", how=\"left\")\n",
        "\n",
        "# 집단통계 결측 보정\n",
        "global_mean = train[TARGET].mean()\n",
        "for col in [\"type_hour_mean\", \"type_wkd_hour_mean\", \"bld_mean\"]:\n",
        "    train[col] = train[col].fillna(global_mean)\n",
        "    test[col]  = test[col].fillna(global_mean)\n",
        "for col in [\"type_hour_std\", \"type_wkd_hour_std\", \"bld_std\"]:\n",
        "    train[col] = train[col].fillna(0.0)\n",
        "    test[col]  = test[col].fillna(0.0)\n",
        "\n",
        "# --- 교호작용: is_weekend × 건물유형(원핫)\n",
        "type_dummies = pd.get_dummies(train[\"건물유형\"], prefix=\"type\")\n",
        "type_cols = type_dummies.columns.tolist()\n",
        "train[type_cols] = type_dummies\n",
        "test[type_cols]  = pd.get_dummies(test[\"건물유형\"], prefix=\"type\").reindex(columns=type_cols, fill_value=0)\n",
        "for c in type_cols:\n",
        "    inter_col = f\"{c}_x_weekend\"\n",
        "    train[inter_col] = train[c] * train[\"is_weekend\"]\n",
        "    test[inter_col]  = test[c]  * test[\"is_weekend\"]\n",
        "\n",
        "\n",
        "# 5) Column curation (+ 앙상블용 메타 보존)\n",
        "meta_train = train[[\"건물번호\",\"건물유형\",\"일시_dt\"]].copy()\n",
        "meta_test  = test[[\"건물번호\",\"건물유형\",\"일시_dt\"]].copy()\n",
        "\n",
        "drop_cols = [\n",
        "    \"일시\",\"건물번호\",\"num_date_time\",\n",
        "    \"강수량(mm)\",\"일조(hr)\",\"일사(MJ/m2)\",\n",
        "    \"태양광용량(kW)\",\"ESS저장용량(kWh)\",\"PCS용량(kW)\",\n",
        "    \"건물유형\"\n",
        "]\n",
        "if \"요일\" in train.columns: drop_cols.append(\"요일\")\n",
        "if \"요일\" in test.columns:  drop_cols.append(\"요일\")\n",
        "\n",
        "train = train.drop(columns=[c for c in drop_cols if c in train.columns])\n",
        "test  = test.drop(columns=[c for c in drop_cols if c in test.columns])\n",
        "\n",
        "y = train[TARGET].values\n",
        "X = train.drop(columns=[TARGET])\n",
        "\n",
        "obj_cols = X.select_dtypes(include=[\"object\"]).columns.tolist()\n",
        "if obj_cols:\n",
        "    print(\"[WARN] object cols dropped:\", obj_cols)\n",
        "    X = X.drop(columns=obj_cols)\n",
        "    test = test.drop(columns=[c for c in obj_cols if c in test.columns])\n",
        "\n",
        "# 6) 홀드아웃 분할\n",
        "VAL_START = pd.to_datetime(\"2024-08-18 00:00:00\")\n",
        "VAL_END   = pd.to_datetime(\"2024-08-24 23:00:00\")\n",
        "\n",
        "mask_tr  = (meta_train[\"일시_dt\"] < VAL_START).values\n",
        "mask_val = ((meta_train[\"일시_dt\"] >= VAL_START) & (meta_train[\"일시_dt\"] <= VAL_END)).values\n",
        "\n",
        "def ensure_no_dt(df):\n",
        "    return df.drop(columns=[c for c in (\"일시_dt\",) if c in df.columns], errors=\"ignore\")\n",
        "\n",
        "X_tr_glob   = ensure_no_dt(X.loc[mask_tr])\n",
        "y_tr_glob   = y[mask_tr]\n",
        "X_val_glob  = ensure_no_dt(X.loc[mask_val])\n",
        "y_val_glob  = y[mask_val]\n",
        "\n",
        "hours_tr = meta_train.loc[mask_tr, \"일시_dt\"].dt.hour.values\n",
        "w_tr = np.where(np.isin(hours_tr, PEAK_HOURS), PEAK_WEIGHT, 1.0)\n",
        "\n",
        "print(f\"Shapes | X_train: {X_tr_glob.shape}  X_val: {X_val_glob.shape}\")\n",
        "\n",
        "# 7) Optuna: XGB 하이퍼파라미터\n",
        "def objective_xgb(trial):\n",
        "    params = {\n",
        "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 300, 900, step=100),\n",
        "        \"max_depth\": trial.suggest_int(\"max_depth\", 4, 10),\n",
        "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.02, 0.10, log=True),\n",
        "        \"subsample\": trial.suggest_float(\"subsample\", 0.6, 1.0),\n",
        "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.6, 1.0),\n",
        "        \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 0.1, 5.0, log=True),\n",
        "        \"min_child_weight\": trial.suggest_float(\"min_child_weight\", 1.0, 10.0),\n",
        "        \"tree_method\": \"hist\",\n",
        "        \"random_state\": SEED,\n",
        "        \"n_jobs\": -1,\n",
        "        \"verbosity\": 0,\n",
        "    }\n",
        "    model = XGBRegressor(**params)\n",
        "    model.fit(X_tr_glob, y_tr_glob, sample_weight=w_tr)\n",
        "    pred = model.predict(X_val_glob)\n",
        "    return smape(y_val_glob, pred)\n",
        "\n",
        "N_TRIALS_XGB = 8  # << 아주 작게\n",
        "print(\"Tuning XGBoost (small trials)...\")\n",
        "study_xgb = optuna.create_study(direction=\"minimize\", study_name=\"xgb_vert_smape\")\n",
        "study_xgb.optimize(objective_xgb, n_trials=N_TRIALS_XGB, show_progress_bar=True)\n",
        "print(\"Best XGB:\", study_xgb.best_value, study_xgb.best_params)\n",
        "\n",
        "best_params = dict(study_xgb.best_params)\n",
        "best_params.update({\"tree_method\":\"hist\", \"random_state\":SEED, \"n_jobs\":-1, \"verbosity\":0})\n",
        "\n",
        "def fit_xgb(X_tr, y_tr, w=None, xgb_params=None):\n",
        "    p = best_params if xgb_params is None else xgb_params\n",
        "    m = XGBRegressor(**p)\n",
        "    m.fit(X_tr, y_tr, sample_weight=w)\n",
        "    return m\n",
        "\n",
        "# 8) 수직 앙상블(전역/유형/건물)\n",
        "\n",
        "# 전역\n",
        "global_model = fit_xgb(X_tr_glob, y_tr_glob, w=w_tr)\n",
        "val_pred_global = global_model.predict(X_val_glob)\n",
        "\n",
        "# 유형별\n",
        "type_tr  = meta_train.loc[mask_tr,  \"건물유형\"].values\n",
        "type_val = meta_train.loc[mask_val, \"건물유형\"].values\n",
        "type_models = {}\n",
        "val_pred_type = np.zeros_like(y_val_glob, dtype=float)\n",
        "\n",
        "for t in np.unique(type_tr):\n",
        "    idx_tr_t = (type_tr == t)\n",
        "    if idx_tr_t.sum() < 50:  # 표본 적으면 스킵\n",
        "        continue\n",
        "    Xm = X_tr_glob.loc[idx_tr_t]\n",
        "    ym = y_tr_glob[idx_tr_t]\n",
        "    wm = w_tr[idx_tr_t]\n",
        "    m = fit_xgb(Xm, ym, wm)\n",
        "    type_models[t] = m\n",
        "    idx_val_t = (type_val == t)\n",
        "    if idx_val_t.sum() > 0:\n",
        "        val_pred_type[idx_val_t] = m.predict(X_val_glob.loc[idx_val_t])\n",
        "\n",
        "fallback = (val_pred_type == 0)\n",
        "if fallback.any():\n",
        "    val_pred_type[fallback] = val_pred_global[fallback]\n",
        "\n",
        "# 건물별\n",
        "bid_tr  = meta_train.loc[mask_tr,  \"건물번호\"].values\n",
        "bid_val = meta_train.loc[mask_val, \"건물번호\"].values\n",
        "bld_models = {}\n",
        "val_pred_bld = np.zeros_like(y_val_glob, dtype=float)\n",
        "\n",
        "for bid in np.unique(bid_tr):\n",
        "    idx_tr_b = (bid_tr == bid)\n",
        "    if idx_tr_b.sum() < 24:\n",
        "        continue\n",
        "    Xm = X_tr_glob.loc[idx_tr_b]\n",
        "    ym = y_tr_glob[idx_tr_b]\n",
        "    wm = w_tr[idx_tr_b]\n",
        "    m = fit_xgb(Xm, ym, wm)\n",
        "    bld_models[bid] = m\n",
        "    idx_val_b = (bid_val == bid)\n",
        "    if idx_val_b.sum() > 0:\n",
        "        val_pred_bld[idx_val_b] = m.predict(X_val_glob.loc[idx_val_b])\n",
        "\n",
        "fallback = (val_pred_bld == 0)\n",
        "if fallback.any():\n",
        "    val_pred_bld[fallback] = val_pred_global[fallback]\n",
        "\n",
        "# 9) Optuna: (전역/유형/건물) 가중치 최적화\n",
        "\n",
        "stack_val = np.vstack([val_pred_global, val_pred_type, val_pred_bld])  # (3, N)\n",
        "\n",
        "def objective_w(trial):\n",
        "    a = trial.suggest_float(\"a\", -2.0, 2.0)\n",
        "    b = trial.suggest_float(\"b\", -2.0, 2.0)\n",
        "    c = trial.suggest_float(\"c\", -2.0, 2.0)\n",
        "    w = np.exp([a,b,c]); w = w/np.sum(w)\n",
        "    pred = w[0]*stack_val[0] + w[1]*stack_val[1] + w[2]*stack_val[2]\n",
        "    return smape(y_val_glob, pred)\n",
        "\n",
        "N_TRIALS_W = 20  # << 아주 작게\n",
        "print(\"Tuning blend weights (small trials)...\")\n",
        "study_w = optuna.create_study(direction=\"minimize\", study_name=\"blend_w_smape\")\n",
        "study_w.optimize(objective_w, n_trials=N_TRIALS_W, show_progress_bar=True)\n",
        "\n",
        "aw, bw, cw = study_w.best_params[\"a\"], study_w.best_params[\"b\"], study_w.best_params[\"c\"]\n",
        "W = np.exp(np.array([aw,bw,cw])); W = W / W.sum()\n",
        "WG, WT, WB = W.tolist()\n",
        "val_pred_ens = WG*val_pred_global + WT*val_pred_type + WB*val_pred_bld\n",
        "print(f\"[VAL] Best SMAPE: {smape(y_val_glob, val_pred_ens):.4f}  | Weights (G/T/B): {WG:.3f}/{WT:.3f}/{WB:.3f}\")\n",
        "\n",
        "# 10) 테스트 예측 (풀데이터 재학습)\n",
        "X_full_glob = ensure_no_dt(X)\n",
        "hours_full  = meta_train[\"일시_dt\"].dt.hour.values\n",
        "w_full      = np.where(np.isin(hours_full, PEAK_HOURS), PEAK_WEIGHT, 1.0)\n",
        "global_model_full = fit_xgb(X_full_glob, y, w=w_full)\n",
        "\n",
        "X_test_glob = ensure_no_dt(test.copy())\n",
        "pred_global_test = global_model_full.predict(X_test_glob)\n",
        "\n",
        "# 유형별 풀데이터\n",
        "type_full = meta_train[\"건물유형\"].values\n",
        "pred_type_test = np.zeros_like(pred_global_test, dtype=float)\n",
        "for t in np.unique(type_full):\n",
        "    idx_full_t = (type_full == t)\n",
        "    if idx_full_t.sum() < 50:\n",
        "        continue\n",
        "    Xm = X_full_glob.loc[idx_full_t]\n",
        "    ym = y[idx_full_t]\n",
        "    wm = w_full[idx_full_t]\n",
        "    m = fit_xgb(Xm, ym, wm)\n",
        "    idx_test_t = (meta_test[\"건물유형\"].values == t)\n",
        "    if idx_test_t.sum() > 0:\n",
        "        pred_type_test[idx_test_t] = m.predict(X_test_glob.loc[idx_test_t])\n",
        "\n",
        "fallback = (pred_type_test == 0)\n",
        "if fallback.any():\n",
        "    pred_type_test[fallback] = pred_global_test[fallback]\n",
        "\n",
        "# 건물별 풀데이터\n",
        "bid_full = meta_train[\"건물번호\"].values\n",
        "pred_bld_test = np.zeros_like(pred_global_test, dtype=float)\n",
        "for bid in np.unique(bid_full):\n",
        "    idx_full_b = (bid_full == bid)\n",
        "    if idx_full_b.sum() < 24:\n",
        "        continue\n",
        "    Xm = X_full_glob.loc[idx_full_b]\n",
        "    ym = y[idx_full_b]\n",
        "    wm = w_full[idx_full_b]\n",
        "    m = fit_xgb(Xm, ym, wm)\n",
        "    idx_test_b = (meta_test[\"건물번호\"].values == bid)\n",
        "    if idx_test_b.sum() > 0:\n",
        "        pred_bld_test[idx_test_b] = m.predict(X_test_glob.loc[idx_test_b])\n",
        "\n",
        "fallback = (pred_bld_test == 0)\n",
        "if fallback.any():\n",
        "    pred_bld_test[fallback] = pred_global_test[fallback]\n",
        "\n",
        "# 최종 블렌딩\n",
        "test_pred_ens = WG*pred_global_test + WT*pred_type_test + WB*pred_bld_test\n",
        "\n",
        "# 11) 제출 파일 생성\n",
        "submission = sub.copy()\n",
        "if \"answer\" in submission.columns:\n",
        "    submission[\"answer\"] = test_pred_ens\n",
        "else:\n",
        "    pred_col = submission.columns[-1]\n",
        "    submission[pred_col] = test_pred_ens\n",
        "\n",
        "OUT_PATH = \"./submission_vert_xgb_optuna.csv\"\n",
        "submission.to_csv(OUT_PATH, index=False, encoding=\"utf-8-sig\")\n",
        "print(submission.head())\n",
        "print(f\"[Saved] {OUT_PATH}\")\n",
        "\n",
        "# 메모리 정리\n",
        "del X_tr_glob, y_tr_glob, X_val_glob, y_val_glob, global_model, global_model_full\n",
        "gc.collect()\n"
      ],
      "metadata": {
        "id": "1fGhe7ZLdVh2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}